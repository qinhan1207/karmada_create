# 十万级节点集群搭建

### 临时设置代理（当前终端有效）

在终端中执行以下命令：

```bash
# 设置HTTP代理
export http_proxy=http://10.71.160.0
export https_proxy=http://10.71.160.0

# 如果需要，设置不使用代理的地址（可选）
export no_proxy=localhost,127.0.0.1,192.168.0.0/16,10.0.0.0/8
```

## 1.k8s集群搭建

现在有三台ubuntu服务器分别为

![image-20250818115659439](C:\Users\30915\AppData\Roaming\Typora\typora-user-images\image-20250818115659439.png)

| 名称         | ip地址        |
| ------------ | ------------- |
| k8s_master   | 192.168.17.11 |
| k8s_worker01 | 192.168.17.12 |
| k8s_work02   | 192.168.17.13 |

### 1.1服务器初始化

#### 1.1.1修改主机名称

以下针对所有主机，每次执行操作都需要保证三台机器都执行了且没有错误。

- k8s-master01

```
hostnamectl set-hostname k8s-master01
```

- k8s-worker01

```
hostnamectl set-hostname k8s-worker01
```

- k8s-worker02

```
hostnamectl set-hostname k8s-worker02
```

#### 1.1.2主机名-IP地址解析

以下针对所有主机，每次执行操作都需要保证三台机器都执行了且没有错误。

```bash
sudo su -
cat >> /etc/hosts << EOF
192.168.17.11 k8s-mater01
192.168.17.12 k8s-worker01
192.168.17.13 k8s-worker02
EOF
exit
```

#### 1.1.3时间同步

以下针对所有主机，每次执行操作都需要保证三台机器都执行了且没有错误。

1. 更换时区

   ```bash
   timedatectl set-timezone Asia/Shanghai
   ```

2. 安装ntpdate

   ```bash
   sudo apt install ntpdate -y
   ```

3. 使用ntpdate命令同步时间

   ```bash
   sudo ntpdate time1.aliyun.com
   ```

4. 通过计划任务实现同步时间

   ```bash
   crontab -e
   ```

​	这里选择输入2，然后在这个文件的最后一行加入下方的cron表达式：

![image-20250818121117216](C:\Users\30915\AppData\Roaming\Typora\typora-user-images\image-20250818121117216.png)

```tex
0 */1 * * * ntpdate time1.aliyun.com
```

#### 1.1.4内核转发、网桥过滤配置

以下针对所有主机，每次执行操作都需要保证三台机器都执行了且没有错误。

配置依赖模块到 `/etc/modules-load.d/k8s.conf` ，后期可开机自动加载：

```bash
sudo -
cat << EOF | tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
exit
```

上面只是开机自动加载，但我们本次还需要用，所以使用下面两条命令进行启用：

```bash
modprobe overlay

modprobe br_netfilter
```

启用完以后，我们可以用下方命令查看是否成功：

```bash
lsmod | egrep "overlay"

lsmod | egrep "br_netfilter"
```

![image-20250818121939287](C:\Users\30915\AppData\Roaming\Typora\typora-user-images\image-20250818121939287.png)

然后我们把网桥过滤和内核转发追加到 `k8s.conf` 文件中：

```bash
cat << EOF | tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
```

最后我们加载内核参数：

```bash
sysctl --system
```

加载完成后，我们可以用下方命令看下内核的路由转发有没有成功打开：

```bash
sysctl -a | grep ip_forward
```

变成 1 了，说明成功了：

![image-20250818122209180](C:\Users\30915\AppData\Roaming\Typora\typora-user-images\image-20250818122209180.png)

#### 1.1.5 安装ipset和ipvsadm

以下针对所有主机，每次执行操作都需要保证三台机器都执行了且没有错误。

首先安装 `ipset` 和 `ipvsadm` ：

（我们后续使用一些负载均衡器的时候需要用到，这个配置是一定要做的）

```bash
sudo apt install ipset ipvsadm -y
```

安装完成后，我们需要配置 `ipvsadm` 模块加载，添加需要加载的模块：（这样我们后续开机就可以自动加载了）

```bash
cat << EOF | tee /etc/modules-load.d/ipvs.conf
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack
EOF
```

上面只是开机自动加载，但我们本次还需要用，所以我们本次要用的话可以将生效命令放到一个脚本文件中，然后执行他：

```
cat << EOF | tee ipvs.sh
#!/bin/sh
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
EOF
```

执行我们刚刚创建的脚本文件：

```
sh ipvs.sh
```

如下图所示，可以看到都加载进来了：

![image-20250818164735259](C:\Users\30915\AppData\Roaming\Typora\typora-user-images\image-20250818164735259.png)

#### 1.1.6关闭SWAP分区

以下针对所有主机，每次执行操作都需要保证三台机器都执行了且没有错误。

永久关闭 `SWAP` 分区：（重启系统才会生效）

用 `vim` 打开 `/etc/fstab`

```
vim /etc/fstab
```

打开后找到 `/swap.img none swap sw 0 0` 这一行，给注释掉：（三台主机都需要操作）

![image-20250818165052975](C:\Users\30915\AppData\Roaming\Typora\typora-user-images\image-20250818165052975.png)

上面的操作，我们重启系统才会生效，我们本次要用，但不想重启系统，所以可以先临时关闭 `SWAP` 分区：

```
swapoff -a
```

然后我们可以用以下命令，查看是否关闭成功：

```
free -m
```

这样就是关闭成功了：

![image-20250818165430313](C:\Users\30915\AppData\Roaming\Typora\typora-user-images\image-20250818165430313.png)

#### 1.1.7禁用防火墙

```
sudo ufw disable
```

### 1.2安装容器运行时

#### 1.2.1安装containerd

所有节点都需执行：

```
# 更新apt包索引
sudo apt update

# 安装必要依赖
sudo apt install -y ca-certificates curl software-properties-common

# 添加Docker官方GPG密钥（containerd属于Docker生态，使用Docker仓库）
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

# 添加Docker仓库
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

# 安装containerd
sudo apt update
sudo apt install -y containerd.io
```

#### 1.2.2 配置 containerd

```bash
# 生成默认配置文件
sudo containerd config default | sudo tee /etc/containerd/config.toml

# 编辑配置文件，设置SystemdCgroup为true（兼容k8s）
sudo sed -i 's/SystemdCgroup \= false/SystemdCgroup \= true/g' /etc/containerd/config.toml

# 重启containerd使配置生效
sudo systemctl restart containerd
sudo systemctl enable containerd
```

### 1.3K8s集群部署

#### 1.3.1软件源准备

```bash
# 添加 Kubernetes 1.31 的官方 APT 仓库
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list

# 下载并安装 GPG 密钥
sudo mkdir -p /etc/apt/keyrings
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
```

#### 1.3.2安装

```
# 更新 APT 并安装 Kubernetes 1.31
sudo apt update
sudo apt install -y kubelet=1.31.* kubectl=1.31.* kubeadm=1.31.*
sudo apt-mark hold kubelet kubeadm kubectl  # 防止自动升级
```

#### 1.3.3为containerd和kubelet配置代理

由于后续需要拉取国外镜像，需配置能够访问外网的代理

##### 1.3.3.1为containerd配置代理

**创建 containerd 服务的代理配置目录**（如果不存在）：

```bash
sudo mkdir -p /etc/systemd/system/containerd.service.d
```

**创建代理配置文件**：

```bash
sudo vim /etc/systemd/system/containerd.service.d/proxy.conf
```

1. **添加代理环境变量**（写入以下内容）：

```conf
[Service]
# 配置 HTTP 代理
Environment="HTTP_PROXY=http://192.168.124.5:7890"
# 配置 HTTPS 代理
Environment="HTTPS_PROXY=http://192.168.124.5:7890"
# 配置无需代理的地址（集群内部地址、本地地址等）
Environment="NO_PROXY=localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,.svc,.cluster.local"
```

**重新加载 systemd 配置并重启 containerd**：

```bash
sudo systemctl daemon-reload
sudo systemctl restart containerd
```

验证配置是否生效

```bash
# 查看 containerd 进程的环境变量，确认代理已生效
sudo cat /proc/$(pgrep containerd)/environ | tr '\0' '\n' | grep -i proxy
```

##### 1.3.3.2为kubelet配置代理

**先创建目录**（确保目录存在）：

```bash
sudo mkdir -p /etc/systemd/system/kubelet.service.d
```

**再创建并编辑文件**：

```bash
sudo vim /etc/systemd/system/kubelet.service.d/proxy.conf
```

**粘贴以下内容**（按 `i` 进入编辑模式，粘贴后按 `ESC`，输入 `:wq` 保存退出）：

```conf
[Service]
Environment="HTTP_PROXY=http://192.168.124.5:7890"
Environment="HTTPS_PROXY=http://192.168.124.5:7890"
Environment="NO_PROXY=localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,.svc,.cluster.local"
```

**重新加载配置并重启 kubelet**：

```bash
sudo systemctl daemon-reload
sudo systemctl restart kubelet
```

这样就能成功配置 kubelet 的代理环境变量了。

#### 1.3.4初始化集群

##### 1.3.4.1在 master 节点执行初始化命令

运行以下命令初始化集群（替换为你的 master 节点实际 IP）：

1. 先预拉取镜像（避免init超时）：

   ```
   sudo http_proxy=http://192.168.124.5:7890 https_proxy=http://192.168.124.5:7890 \
   kubeadm config images pull \
     --image-repository=registry.aliyuncs.com/google_containers \
     --kubernetes-version=v1.31.12
   ```

2. 执行初始化：

   ```
   sudo http_proxy=http://192.168.124.5:7890 https_proxy=http://192.168.124.5:7890 \
   kubeadm init \
     --apiserver-advertise-address=192.168.17.11 \
     --image-repository=registry.aliyuncs.com/google_containers \
     --kubernetes-version=v1.31.12 \
     --service-cidr=10.96.0.0/12 \
     --pod-network-cidr=192.168.0.0/16 \
     --upload-certs
   ```

3. 配置kubectl权限

   ```bash
   mkdir -p $HOME/.kube
   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
   sudo chown $(id -u):$(id -g) $HOME/.kube/config
   ```

4. 安装Calico：

   ```bash
   # 手动下载calico yaml文件
   sudo http_proxy=http://192.168.124.5:7890 https_proxy=http://192.168.124.5:7890 \
   curl -fSL -O https://docs.projectcalico.org/v3.25/manifests/calico.yaml
   # 应用calico
   kubectl apply -f calico.yaml
   ```


##### 1.3.4.2将两个工作节点加入主节点（在两个工作节点执行）

这个命令为kubeadm init执行成功后所生成的命令：

```bash
sudo kubeadm join 192.168.17.11:6443 --token 2fow85.c16mozahj0sbwsx1 \
  --discovery-token-ca-cert-hash sha256:fd219afdc5150aa38e464ab749cafe73c2577cdf71d33c98b832f62292a58eb7
```

## 2.在k8s集群上部署karmada

### 2.1在k8s主节点安装helm

下列操作需要在主节点执行

```bash
sudo snap install helm --classic
```

安装完成后，同样可以通过 `helm version` 验证是否安装成功：

```bash
helm version
```

### 2.2添加karmada chat repo

```bash
helm repo add karmada-charts https://raw.githubusercontent.com/karmada-io/karmada/master/charts
# 查看添加是否成功
helm repo list
# 搜索karmada相关版本
helm search repo karmada
```

![image-20250819152126931](C:\Users\30915\AppData\Roaming\Typora\typora-user-images\image-20250819152126931.png)

### 2.3安装karmada

由于需要从github拉取镜像，可能需要为终端配置代理

```bash
# 创建命名空间
kubectl create namespace karmada-system

# 安装 Karmada（这里版本需要与上面命令查出来的版本对应）
helm install karmada karmada-charts/karmada -n karmada-system --version v1.14.0

# 验证安装
kubectl get pods -n karmada-system
```

### 2.4获取karmada的kubeconfig

```bash
# 提取 kubeconfig 并保存到文件
kubectl get secret -n karmada-system karmada-kubeconfig -o jsonpath='{.data.kubeconfig}' | base64 -d > karmada-config
```

### 2.5下载对应版本的karmadactl

```bash
# 查看karmada版本
helm list -n karmada-system
# 下载
wget https://github.com/karmada-io/karmada/releases/download/v1.14.0/karmadactl-linux-amd64.tgz
tar -zxvf karmadactl-linux-amd64.tgz
sudo mv karmadactl /usr/local/bin/
# 验证版本一致性
karmadactl version
```

### 2.6使用kubectl操作集群

```bash
kubectl get clusters --kubeconfig=/home/qinhan/karmada-config
```

如果出现问题以下问题

![image-20250823150935527](C:\Users\30915\AppData\Roaming\Typora\typora-user-images\image-20250823150935527.png)

执行以下命令：

```
# 检查 CoreDNS pods 是否正常运行
kubectl get pods -n kube-system -l k8s-app=kube-dns

# 获取 CoreDNS 服务的 ClusterIP
kubectl get svc -n kube-system -l k8s-app=kube-dns

# 假设 CoreDNS 的 ClusterIP 是 10.96.0.10（常见默认值），根据上面一条命令查出来的clusterIp
nslookup karmada-apiserver.karmada-system.svc.cluster.local 10.96.0.10
```

以下图片表明coreDNS正常

![image-20250823151354111](C:\Users\30915\AppData\Roaming\Typora\typora-user-images\image-20250823151354111.png)

需要修改本地DNS

```bash
# 编辑 resolved.conf
sudo vi /etc/systemd/resolved.conf

# 添加以下内容
[Resolve]
DNS=10.96.0.10  # CoreDNS 的 ClusterIP
Domains=~cluster.local ~svc.cluster.local ~karmada-system.svc.cluster.local
LLMNR=no
MulticastDNS=no
DNSSEC=no

# 重新加载配置
sudo systemctl daemon-reload
sudo systemctl restart systemd-resolved
sudo resolvectl flush-caches
```

### 2.7卸载karmada

```
helm uninstall karmada -n karmada-system
kubectl delete namespace karmada-system --ignore-not-found
```

## 3.使用k3d或者kwok来模拟集群和节点

### 3.1准备一台服务器，做初始化配置

#### 3.1.1安装docker

```bash
# 安装 Docker（k3d 依赖 Docker）
curl -fsSL https://get.docker.com | sh

# 刷新用户组权限
newgrp docker

# 设置开机自启
sudo systemctl enable --now docker

# 为docker配置代理，方便拉取镜像
# 创建 Docker 服务配置目录（如果不存在）
sudo mkdir -p /etc/systemd/system/docker.service.d

# 创建代理配置文件
sudo vim /etc/systemd/system/docker.service.d/proxy.conf
```

在打开的文件中添加以下内容（注意替换为你的代理地址）：

```ini
[Service]
Environment="HTTP_PROXY=http://192.168.124.5:7890/"
Environment="HTTPS_PROXY=http://192.168.124.5:7890/"
Environment="NO_PROXY=localhost,127.0.0.1,192.168.0.0/16,10.0.0.0/8,.docker.internal"
```

重新加载配置并重启docker

```bash
# 重新加载系统服务配置
sudo systemctl daemon-reload

# 重启 Docker 服务
sudo systemctl restart docker
```

验证代理是否生效

```bash
# 查看 Docker 服务的环境变量，确认代理配置已加载
sudo systemctl show --property=Environment docker
```

### 3.2 

#### 3.1.2安装k3d

```bash
# 安装 k3d（K3s 轻量化集群工具）
curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash
```

#### 3.1.3安装kubelet

```bash
# 安装 kubectl（Kubernetes 命令行工具）
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
```

### 3.2使用k3d来创建模拟集群

```bash
# 
k3d cluster create mycluster1 \
  --api-port 192.168.17.21:6443   \
  --env "HTTP_PROXY=http://192.168.124.5:7890@server:*"   \
  --env "HTTPS_PROXY=http://192.168.124.5:7890@server:*"   \
  --env "NO_PROXY=localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,.svc,.cluster.local@server:*"
  
# 查看创建的集群
k3d cluster list

# 删除
k3d cluster delete mycluster
  
 # 获取 mycluster1 的 kubeconfig，并检查节点
k3d kubeconfig get worker-cluster-1 > ~/cluster1-config

kubectl get nodes
```

### 3.3模拟大量节点

#### 3.3.1在 K3d 集群中部署 KWOK

```bash
# 确保 KUBECONFIG 指向目标集群 (例如 worker-cluster-1)
export KUBECONFIG=~/cluster1-config

# 部署 KWOK
kubectl apply -f https://github.com/kubernetes-sigs/kwok/releases/download/v0.5.0/kwok.yaml

# 等待控制器就绪
kubectl get po -l app=kwok-controller -n kube-system
```

#### 3.3.2安装kwokctl

- 下载 `kwokctl` 二进制文件

在终端中执行 `wget` 命令（如果 `wget` 不可用，也可以使用 `curl -LO` 命令 ），将 `kwokctl` 下载到当前用户目录下：

```bash
wget https://github.com/kubernetes-sigs/kwok/releases/download/v0.7.0/kwokctl-linux-amd64
```

-  赋予执行权限

下载完成后，给 `kwokctl` 二进制文件赋予可执行权限，执行以下命令：

```bash
chmod +x kwokctl-linux-amd64
```

- 3. 移动到系统可执行路径

将 `kwokctl` 移动到系统全局可执行路径 `/usr/local/bin` 下，这样在任何目录都能直接使用 `kwokctl` 命令，执行：

```bash
sudo mv kwokctl-linux-amd64 /usr/local/bin/kwokctl
```

- 验证安装是否成功

在终端中输入 `kwokctl --version` ，如果能正常输出版本信息（比如显示 `kwokctl version v0.7.0` ），则说明 `kwokctl` 已经成功安装到你的系统中。

#### 3.2.3用 `kwokctl` 导入现有集群（关键步骤）

`kwokctl` 管理外部集群（如 `k3d` 集群）的核心命令是 `kwokctl create cluster --import`，通过该命令将 `KUBECONFIG` 中的集群 “注册” 到 `kwokctl` 中：

```bash
# 格式：kwokctl create cluster --name <自定义集群名> --kubeconfig <你的 k3d 配置文件路径>
kwokctl create cluster --name my-k3d-cluster --kubeconfig ~/cluster1-config
```

- `--name my-k3d-cluster`：给集群起一个自定义名称（后续用这个名称操作集群，比如 `kwokctl scale node --name my-k3d-cluster`）；
- `--import`：表示 “导入现有集群”，而非创建新集群；
- `--kubeconfig ~/cluster1-config`：指定要导入的集群配置文件。

预期成功输出：

```plaintext
✅ Cluster "my-k3d-cluster" imported successfully
✅ Kubeconfig verified and linked to cluster
```

第三步：验证 `kwokctl` 已识别集群

执行以下命令，此时 `kwokctl` 就能显示导入的集群了：

```bash
kwokctl get clusters
```

预期输出（能看到集群名称、状态、K8s 版本）：

```plaintext
NAME              STATUS    KUBERNETES-VERSION
my-k3d-cluster    Running   v1.28.6  # 版本与你的 k3d 集群一致
```

第四步：用 `kwokctl` 批量创建模拟节点

集群导入成功后，即可直接用 `kwokctl` 操作，无需重复指定 `kubeconfig`（若未导入，需每次加 `--kubeconfig` 参数）：

```bash
# 示例：创建 10 个模拟节点（--name 指定导入的集群名）
kwokctl scale node --name my-k3d-cluster --replicas 10

# 验证结果（用 kubectl 查看新增的模拟节点）
kubectl --kubeconfig ~/cluster1-config get nodes
```



- 预期能看到 10 个以 `kwok-node-xxx` 命名的模拟节点，状态为 `Ready`。

关键说明

`kwokctl` 设计上区分 “自己创建的集群” 和 “外部集群（如 k3d、kind）”：

- 若用 `kwokctl create cluster` 直接创建集群，它会自动管理配置；
- 若要管理外部集群（如你的 k3d 集群），必须通过 `--import` 参数明确导入，否则无法识别 —— 这是之前显示 `No clusters found` 的核心原因。

如果导入后仍有问题，可执行 `kwokctl get clusters --kubeconfig ~/cluster1-config` 排查，或告诉我具体错误提示，进一步分析！

### 3.4使用kwok创建集群并模拟节点

### 3.4.1安装kwok

- 下载 `kwokctl` 二进制文件

在终端中执行 `wget` 命令（如果 `wget` 不可用，也可以使用 `curl -LO` 命令 ），将 `kwokctl` 下载到当前用户目录下：

```bash
wget https://github.com/kubernetes-sigs/kwok/releases/download/v0.7.0/kwokctl-linux-amd64
```

-  赋予执行权限

下载完成后，给 `kwokctl` 二进制文件赋予可执行权限，执行以下命令：

```bash
chmod +x kwokctl-linux-amd64
```

- 3. 移动到系统可执行路径

将 `kwokctl` 移动到系统全局可执行路径 `/usr/local/bin` 下，这样在任何目录都能直接使用 `kwokctl` 命令，执行：

```bash
sudo mv kwokctl-linux-amd64 /usr/local/bin/kwokctl
```

- 验证安装是否成功

在终端中输入 `kwokctl --version` ，如果能正常输出版本信息（比如显示 `kwokctl version v0.7.0` ），则说明 `kwokctl` 已经成功安装到你的系统中。

### 3.4.2使用kwok创建集群

**创建集群**

```bash
# 创建一个默认配置的集群（默认名称为 "kwok"）
kwokctl create cluster

# 创建指定名称的集群
kwokctl create cluster --name my-kwok-cluster

# 创建指定 Kubernetes 版本的集群
kwokctl create cluster --version v1.27.0
```

**导出 kubeconfig 文件**

```bash
# 导出 kubeconfig 到指定文件
kwokctl get kubeconfig --name my-kwok-cluster > ./my-kubeconfig.yaml

```

**集群节点扩容**

```bash
# 增加指定数量的节点（默认节点类型为 worker）
kwokctl scale node --replicas 3

# 增加指定名称的节点
kwokctl scale node --nodes node-4,node-5

# 增加控制平面节点
kwokctl scale node --control-plane --replicas 2
```

**集群节点缩容**

```bash
# 减少节点到指定数量（保留指定数量的节点）
kwokctl scale node --replicas 1

# 删除指定节点
kwokctl delete node node-2 node-3
```

**删除集群**

```bash
# 删除默认集群
kwokctl delete cluster

# 删除指定名称的集群
kwokctl delete cluster --name my-kwok-cluster

# 强制删除集群（处理残留资源）
kwokctl delete cluster --force
```

**其他常用辅助命令**

```bash
# 列出所有集群
kwokctl get clusters

# 查看集群状态
kwokctl describe cluster

# 启动集群
kwokctl start cluster

# 停止集群
kwokctl stop cluster
```

这些命令覆盖了 KWOK 集群的完整生命周期管理，从创建到销毁，以及中间的配置导出和节点扩缩容操作。使用时可以通过 `kwokctl [command] --help` 查看每个命令的详细参数说明。

## 4.加入集群

删除集群命令

```
kubectl delete cluster kwok-test --kubeconfig=/home/qinhan/karmada-config

kubectl delete cluster kwok-test --kubeconfig=/home/qinhan/karmada-config --force --grace-period=0

# 移除 finalizers
kubectl patch cluster kwok-test --kubeconfig=/home/qinhan/karmada-config --type='json' -p='[{"op": "remove", "path": "/metadata/finalizers"}]'
```

